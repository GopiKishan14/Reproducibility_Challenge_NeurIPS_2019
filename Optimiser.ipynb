{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Conjugate function for Mx = b solving\"\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "\n",
    "def Hvp_vec(grad_vec, params, vec, retain_graph=False):\n",
    "    if torch.isnan(grad_vec).any():\n",
    "        print('grad vec nan')\n",
    "        raise ValueError('grad Nan')\n",
    "    if torch.isnan(vec).any():\n",
    "        print('vec nan')\n",
    "        raise ValueError('vec Nan')\n",
    "    try:\n",
    "        grad_grad = autograd.grad(grad_vec, params, grad_outputs=vec, retain_graph=retain_graph, allow_unused=True)\n",
    "        hvp = torch.cat([g.contiguous().view(-1) for g in grad_grad])\n",
    "        if torch.isnan(hvp).any():\n",
    "            print('hvp nan')\n",
    "            raise ValueError('hvp Nan')\n",
    "    except:\n",
    "        # print('filling zero for None')\n",
    "        grad_grad = autograd.grad(grad_vec, params, grad_outputs=vec, retain_graph=retain_graph,\n",
    "                                  allow_unused=True)\n",
    "        grad_list = []\n",
    "        for i, p in enumerate(params):\n",
    "            if grad_grad[i] is None:\n",
    "                grad_list.append(torch.zeros_like(p))\n",
    "            else:\n",
    "                grad_list.append(grad_grad[i].contiguous().view(-1))\n",
    "        hvp = torch.cat(grad_list)\n",
    "        if torch.isnan(hvp).any():\n",
    "            raise ValueError('hvp Nan')\n",
    "    return hvp\n",
    "\n",
    "def zero_grad(params):\n",
    "    for p in params:\n",
    "        if p.grad is not None:\n",
    "            p.grad.detach()\n",
    "            p.grad.zero_()\n",
    "\n",
    "def conjugate_gradient(grad_x, grad_y, x_params, y_params, b, x=None, nsteps=10, residual_tol=1e-18,\n",
    "                       lr=1.0, device=torch.device('cpu')):\n",
    "    '''\n",
    "    :param grad_x:\n",
    "    :param grad_y:\n",
    "    :param x_params:\n",
    "    :param y_params:\n",
    "    :param b: vec\n",
    "    :param nsteps: max number of steps\n",
    "    :param residual_tol:\n",
    "    :return: A ** -1 * b\n",
    "    h_1 = D_yx * p\n",
    "    h_2 = D_xy * D_yx * p\n",
    "    A = I + lr ** 2 * D_xy * D_yx * p\n",
    "    '''\n",
    "    if x is None:\n",
    "        x = torch.zeros(b.shape[0], device=device)\n",
    "    r = b.clone().detach()\n",
    "    p = r.clone().detach()\n",
    "    rdotr = torch.dot(r, r)\n",
    "    residual_tol = residual_tol * rdotr\n",
    "    \n",
    "    i = 0\n",
    "    for i in range(nsteps):\n",
    "        # To compute Avp\n",
    "        h_1 = Hvp_vec(grad_vec=grad_x, params=y_params, vec=p, retain_graph=True,allow_unused=True)\n",
    "        h_2 = Hvp_vec(grad_vec=grad_y, params=x_params, vec=h_1, retain_graph=True, allow_unused=True)\n",
    "        Avp_ = p + lr * lr * h_2\n",
    "\n",
    "        alpha = rdotr / torch.dot(p, Avp_)\n",
    "        x.data.add_(alpha * p)\n",
    "        r.data.add_(- alpha * Avp_)\n",
    "        new_rdotr = torch.dot(r, r)\n",
    "        beta = new_rdotr / rdotr\n",
    "        p = r + beta * p\n",
    "        rdotr = new_rdotr\n",
    "        if rdotr < residual_tol:\n",
    "            break\n",
    "    return x, i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class BCGD(object):\n",
    "    def __init__(self, max_params, min_params, lr=1e-3, weight_decay=0, device=torch.device('cpu'),\n",
    "                 solve_x=False, collect_info=True):\n",
    "        self.max_params = max_params\n",
    "        self.min_params = min_params\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.device = device\n",
    "        self.solve_x = solve_x\n",
    "        self.collect_info = collect_info\n",
    "\n",
    "        self.old_x = None\n",
    "        self.old_y = None\n",
    "\n",
    "    def zero_grad(self):\n",
    "        zero_grad(self.max_params)\n",
    "        zero_grad(self.min_params)\n",
    "\n",
    "    def getinfo(self):\n",
    "        if self.collect_info:\n",
    "            return self.norm_gx, self.norm_gy, self.norm_px, self.norm_py, self.norm_cgx, self.norm_cgy, \\\n",
    "                   self.timer, self.iter_num\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                'No update information stored. Set collect_info=True before call this method')\n",
    "\n",
    "    def step(self, loss):\n",
    "        grad_x = autograd.grad(loss, self.max_params, create_graph=True, retain_graph=True, allow_unused=True)\n",
    "        # print(grad_x)\n",
    "        grad_x_vec = torch.cat([g.contiguous().view(-1) for g in grad_x])\n",
    "        grad_y = autograd.grad(loss, self.min_params, create_graph=True, retain_graph=True, allow_unused=True)\n",
    "        grad_y_vec = torch.cat([g.contiguous().view(-1) for g in grad_y])\n",
    "\n",
    "        hvp_x_vec = Hvp_vec(grad_y_vec, self.max_params, grad_y_vec,\n",
    "                            retain_graph=True)  # h_xy * d_y\n",
    "        hvp_y_vec = Hvp_vec(grad_x_vec, self.min_params, grad_x_vec,\n",
    "                            retain_graph=True)  # h_yx * d_x\n",
    "\n",
    "        p_x = torch.add(grad_x_vec, - self.lr * hvp_x_vec)\n",
    "        p_y = torch.add(grad_y_vec, self.lr * hvp_y_vec)\n",
    "        if self.collect_info:\n",
    "            self.norm_px = torch.norm(p_x, p=2)\n",
    "            self.norm_py = torch.norm(p_y, p=2)\n",
    "            self.timer = time.time()\n",
    "\n",
    "\n",
    "        # # solve both x and y\n",
    "\n",
    "        # # solve_x\n",
    "        # cg_y, self.iter_num = conjugate_gradient(grad_x=grad_y_vec, grad_y=grad_x_vec,\n",
    "        #                                              x_params=self.min_params,\n",
    "        #                                              y_params=self.max_params, b=p_y, x=self.old_y,\n",
    "        #                                              nsteps=p_y.shape[0] // 10000,\n",
    "        #                                              lr=self.lr, device=self.device)\n",
    "        # hcg = Hvp_vec(grad_y_vec, self.max_params, cg_y)\n",
    "        # cg_x = torch.add(grad_x_vec, - self.lr * hcg)\n",
    "        # self.old_x = cg_x\n",
    "\n",
    "\n",
    "        # # solve_y\n",
    "        # cg_x, self.iter_num = conjugate_gradient(grad_x=grad_x_vec, grad_y=grad_y_vec,\n",
    "        #                                              x_params=self.max_params,\n",
    "        #                                              y_params=self.min_params, b=p_x, x=self.old_x,\n",
    "        #                                              nsteps=p_x.shape[0] // 10000,\n",
    "        #                                              lr=self.lr, device=self.device)\n",
    "        # hcg = Hvp_vec(grad_x_vec, self.min_params, cg_x)\n",
    "        # cg_y = torch.add(grad_y_vec, self.lr * hcg)\n",
    "        # self.old_y = cg_y\n",
    "\n",
    "\n",
    "        if self.solve_x:\n",
    "            cg_y, self.iter_num = conjugate_gradient(grad_x=grad_y_vec, grad_y=grad_x_vec,\n",
    "                                                     x_params=self.min_params,\n",
    "                                                     y_params=self.max_params, b=p_y, x=self.old_y,\n",
    "                                                     nsteps=p_y.shape[0] // 10000,\n",
    "                                                     lr=self.lr, device=self.device)\n",
    "            hcg = Hvp_vec(grad_y_vec, self.max_params, cg_y)\n",
    "            cg_x = torch.add(grad_x_vec, - self.lr * hcg)\n",
    "            self.old_x = cg_x\n",
    "        else:\n",
    "            cg_x, self.iter_num = conjugate_gradient(grad_x=grad_x_vec, grad_y=grad_y_vec,\n",
    "                                                     x_params=self.max_params,\n",
    "                                                     y_params=self.min_params, b=p_x, x=self.old_x,\n",
    "                                                     nsteps=p_x.shape[0] // 10000,\n",
    "                                                     lr=self.lr, device=self.device)\n",
    "            hcg = Hvp_vec(grad_x_vec, self.min_params, cg_x)\n",
    "            cg_y = torch.add(grad_y_vec, self.lr * hcg)\n",
    "            self.old_y = cg_y\n",
    "\n",
    "        if self.collect_info:\n",
    "            self.timer = time.time() - self.timer\n",
    "\n",
    "        index = 0\n",
    "        for p in self.max_params:\n",
    "            if self.weight_decay != 0:\n",
    "                p.data.add_(- self.weight_decay * p)\n",
    "            p.data.add_(self.lr * cg_x[index: index + p.numel()].reshape(p.shape))\n",
    "            index += p.numel()\n",
    "        if index != cg_x.numel():\n",
    "            raise ValueError('CG size mismatch')\n",
    "        index = 0\n",
    "        for p in self.min_params:\n",
    "            if self.weight_decay != 0:\n",
    "                p.data.add_(- self.weight_decay * p)\n",
    "            p.data.add_(- self.lr * cg_y[index: index + p.numel()].reshape(p.shape))\n",
    "            index += p.numel()\n",
    "        if index != cg_y.numel():\n",
    "            raise ValueError('CG size mismatch')\n",
    "\n",
    "        if self.collect_info:\n",
    "            self.norm_gx = torch.norm(grad_x_vec, p=2)\n",
    "            self.norm_gy = torch.norm(grad_y_vec, p=2)\n",
    "            self.norm_cgx = torch.norm(cg_x, p=2)\n",
    "            self.norm_cgy = torch.norm(cg_y, p=2)\n",
    "        self.solve_x = False if self.solve_x else True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([0.5], requires_grad=True) # 1D vector max_param\n",
    "y = torch.tensor([0.5], requires_grad=True) # 1D vector min_param\n",
    "\n",
    "# Hyperparameter\n",
    "eta = 0.2  #learning rate\n",
    "gamma = 1.0 # consesus rate\n",
    "alpha = 1.0\n",
    "\n",
    "# Bilinear Game \n",
    "\n",
    "## Test case1\n",
    "f = alpha * torch.dot(x , y)  # opt function\n",
    "g = -1*alpha * torch.dot(x , y) \n",
    "\n",
    "\n",
    "# ## Test case 2\n",
    "\n",
    "# f = alpha * (torch.dot(x , x) - torch.dot(y,y))\n",
    "# g=  -1*alpha * (torch.dot(x , x) - torch.dot(y,y))\n",
    "\n",
    "\n",
    "## Test Case 3\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "solver = BCGD(x, y)\n",
    "epoch = 10\n",
    "\n",
    "steps = [0.5]\n",
    "\n",
    "for e in range(epoch):\n",
    "    solver.step(loss= f)\n",
    "\n",
    "    steps.append(solver.old_y)\n",
    "\n",
    "plt.plot(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most code source taken from :\n",
    "\n",
    "```\n",
    "@misc{schfer2019implicit,\n",
    "    title={Implicit competitive regularization in GANs},\n",
    "    author={Florian Schäfer and Hongkai Zheng and Anima Anandkumar},\n",
    "    year={2019},\n",
    "    eprint={1910.05852},\n",
    "    archivePrefix={arXiv},\n",
    "    primaryClass={cs.LG}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
